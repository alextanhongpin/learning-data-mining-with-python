{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OneR \n",
    "\n",
    "OneR is a shorthand of One Rule, indicating that we only use a single rule for this classification by choosing the feature with the best performance.\n",
    "\n",
    "How it works:\n",
    "- iterate every value of the feature\n",
    "- for that value, count the number of samples for each class that have that feature value\n",
    "- record the most frequent class for the feature value, and the error of that prediction\n",
    "- compute the error for each feature by summing up the errors for all the values for that feature. The feature with the lowest total error is chosen as the One Rule and then used to classify other instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.84333333, 3.05733333, 3.758     , 1.19933333])"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the mean of each feature - flatten all rows into a single row.\n",
    "X_mean = X.mean(axis=0)\n",
    "X_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 1, 0, 0]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_d = np.array(X >= X_mean, dtype=np.int)\n",
    "X_d[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_value(X, y, i, value):\n",
    "    \"\"\"\n",
    "    Find the samples at the given feature with the given feature\n",
    "    values, and computes the errors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array[n_samples, n_features]\n",
    "        The two dimensional ...\n",
    "    \"\"\"\n",
    "    # Target only the feature column.\n",
    "    X_i = X[:, i]\n",
    "    \n",
    "    # Get samples with the given feature column value. \n",
    "    # Select the classes values.\n",
    "    y_Xi = y[X_i == value]\n",
    "    \n",
    "    # Calculate the class values.\n",
    "    counter = Counter(y_Xi)\n",
    "    \n",
    "    # Select the most frequent class.\n",
    "    most_frequent_class, count = counter.most_common(1)[0]\n",
    "    \n",
    "    # The errors are simply the values that are not frequent.\n",
    "    error = len(y_Xi) - count\n",
    "    return most_frequent_class, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, feature_index):\n",
    "    \"\"\"\n",
    "    Computes the predictors and error for a given feature using OneR algorithm.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array[n_samples, n_features]\n",
    "        The two dimensional array that holds the data set. \n",
    "        Each row is a sample, each column is a feature.\n",
    "    y: array[n_samples,]\n",
    "        The one dimensional array that holds the class values. \n",
    "        Corresponds to X, such that y[i] is the class value\n",
    "        for sample X[i].\n",
    "    feature: int\n",
    "        An integer corresponding to the index of the variable we wish\n",
    "        to test.\n",
    "        0 <= feature < n_features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictors: dictionary of tuples: (value, prediction)\n",
    "        For each item in the array, if the variable has a\n",
    "        given value, make the given prediction.\n",
    "    error: float\n",
    "        The ratio of training data that this rule incorrectly predicts.\n",
    "    \"\"\"\n",
    "    # Get all unique values that this variable has.\n",
    "    values = np.unique(X[:, feature_index])\n",
    "    predictors = {}\n",
    "    errors = []\n",
    "    \n",
    "    for current_value in values:\n",
    "        most_frequent_class, error = train_feature_value(X, y, feature_index, current_value)\n",
    "        predictors[current_value] = most_frequent_class\n",
    "        errors.append(error)\n",
    "    total_error = sum(errors)\n",
    "    return predictors, total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_d, y, random_state=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictors = {}\n",
    "errors = {}\n",
    "_, n_features = X_train.shape\n",
    "for feature_index in range(n_features):\n",
    "    predictors, total_error = train(X_train, y_train, feature_index)\n",
    "    all_predictors[feature_index] = (predictors, total_error)\n",
    "    errors[feature_index] = total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 37)"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_feature, best_error = sorted(errors.items(), \n",
    "                                  key=lambda t: t[1])[0] # Sort by values, lowest error.\n",
    "best_feature, best_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'variable': 2, 'predictor': {0: 0, 1: 2}}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = {\n",
    "    'variable': best_feature,\n",
    "    'predictor': all_predictors[best_feature][0]\n",
    "}\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    variable = model['variable']\n",
    "    predictor = model['predictor']\n",
    "    y_predicted = np.array([predictor[int(sample[variable])]\n",
    "                            for sample in X_test])\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 2, 2, 2, 0, 2, 0, 2, 2, 0, 2, 2, 0, 2, 0, 2, 2, 2, 0, 0,\n",
       "       0, 2, 0, 2, 0, 2, 2, 0, 0, 0, 2, 0, 2, 0, 2, 2])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_test, model)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.78947368421053"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.mean(y_pred == y_test) * 100\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        17\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.40      1.00      0.57         8\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.45      0.67      0.51        38\n",
      "weighted avg       0.51      0.66      0.55        38\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
