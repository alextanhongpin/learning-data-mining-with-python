{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One Rule** algorithm is a simple simple algorithm that simply predicts the class of a sample by finding the most frequent class for the feature values. We only use a single rule for this classification by choose the feature with the best performance (fewest prediction errors). If both attributes produces the same score, then OneR chooses the attributes at random.\n",
    "\n",
    "The algorithm is as follow:\n",
    "\n",
    "for each attribute `A`:\n",
    "\n",
    "    for each value `v` of that attribute, create a rule:\n",
    "        1. count how often each class appears\n",
    "        2. find the most frequent class, `C`\n",
    "        3. make a rule, if `A=v`, then `C=c`\n",
    "calculate the error rate of this value\n",
    "find the attribute that produces the lowest error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Plants Database\n",
      "====================\n",
      "\n",
      "Notes\n",
      "-----\n",
      "Data Set Characteristics:\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML iris datasets.\n",
      "http://archive.ics.uci.edu/ml/datasets/Iris\n",
      "\n",
      "The famous Iris database, first used by Sir R.A Fisher\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      "References\n",
      "----------\n",
      "   - Fisher,R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda,R.O., & Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "dataset = load_iris()\n",
    "print(dataset.DESCR)\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "n_samples, n_features = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the mean for each attributes\n",
    "attribute_means = X.mean(axis=0)\n",
    "assert attribute_means.shape == (n_features,)\n",
    "\n",
    "X_d = np.array(X >= attribute_means, dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are (112,) training samples\n",
      "There are (38,) test samples\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Seed our random state so that we will get reproducible results\n",
    "random_state = 14\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_d, y, random_state=random_state)\n",
    "\n",
    "print(\"There are {} training samples\".format(y_train.shape))\n",
    "print(\"There are {} test samples\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "def train_feature_value(X, y_true, feature, value):\n",
    "    class_counts = defaultdict(int)\n",
    "    \n",
    "    for sample, y in zip(X, y_true):\n",
    "        if sample[feature] == value:\n",
    "            class_counts[y] += 1\n",
    "    \n",
    "    sorted_class_counts = sorted(class_counts.items(), key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    most_frequent_class = sorted_class_counts[0][0]\n",
    "    \n",
    "    incorrect_predictions = [class_count for class_value, class_count in class_counts.items() \n",
    "                             if class_value != most_frequent_class]\n",
    "    \n",
    "    error = sum(incorrect_predictions)\n",
    "    \n",
    "    return most_frequent_class, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y_true, feature):\n",
    "    \"\"\"Compute the predictors and error for a given feature using the OneR algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: array [n_samples, n_features]\n",
    "        The two dimensional array that holds the dataset. Each row is a sample, each column\n",
    "        is a feature.\n",
    "        \n",
    "    y_true: array[n_samples,]\n",
    "        The one dimensional array that holds the class values. Corresponds to X, such that\n",
    "        y_true[i] is the class value for sample X[i]\n",
    "    \n",
    "    feature: int\n",
    "        An integer corresponding to the index of the variables we wish to test.\n",
    "        0 <= variable < n_features\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictors: dictionary of tuples: (value, prediction)\n",
    "        For each item in the array, if the variable has a given value, make the given prediction.\n",
    "    \n",
    "    error: float\n",
    "        The ratio of training data that this rule incorrectly predicts.\n",
    "    \"\"\"\n",
    "\n",
    "    values = set(X[:, feature])\n",
    "    predictors = {}\n",
    "    errors = []\n",
    "    \n",
    "    for current_value in values:\n",
    "        most_frequent_class, error = train_feature_value(X, y_true, feature, current_value)\n",
    "        predictors[current_value] = most_frequent_class\n",
    "        errors.append(error)\n",
    "    \n",
    "    total_error = sum(errors)\n",
    "    return predictors, total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feature': 0, 'predictor': {0: 0, 1: 2}}\n"
     ]
    }
   ],
   "source": [
    "all_predictors = {}\n",
    "errors = {}\n",
    "\n",
    "for feature in range(X_train.shape[1]):\n",
    "    predictors, total_error = train(X_train, y_train, feature_index)\n",
    "    all_predictors[feature] = (predictors, total_error)\n",
    "    errors[feature] = total_error\n",
    "\n",
    "best_feature, best_error = sorted(errors.items(), key=itemgetter(1))[0]\n",
    "\n",
    "model = {'feature': best_feature,\n",
    "         'predictor': all_predictors[best_feature][0]}\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(X_test, model):\n",
    "    \"\"\"\n",
    "    Predict the best category given an array of test features\n",
    "    \n",
    "    Arguments:\n",
    "    - X_test (ndarray) : an array of test variables\n",
    "    - model (object)   : an object that holds the best feature and all predictors \n",
    "    \n",
    "    Returns:\n",
    "    - y_predicted (ndarray) : an array of predicted output\n",
    "    \"\"\"\n",
    "\n",
    "    feature = model['feature']\n",
    "    predictor = model['predictor']\n",
    "    y_predicted = np.array([predictor[int(sample[feature])] for sample in X_test])\n",
    "    return y_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 2 0 0 0 2 0 0 2 2 0 0 0 2 2 0 0 0 0 2 0 2 0 0 0 0 0 0 2 0 0 0 2\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "y_predicted = predict(X_test, model)\n",
    "print(y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is 60.5%\n"
     ]
    }
   ],
   "source": [
    "accuracy = np.mean(y_predicted == y_test) * 100\n",
    "print('The test accuracy is {:.1f}%'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
